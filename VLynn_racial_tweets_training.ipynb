{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-label classification of racial tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/valery/Documents/Springboard/Capstone_2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get directory path\n",
    "import os\n",
    "os.chdir('/Users/valery/Documents/Springboard/Capstone_2') \n",
    "os.getcwd( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6481, 5)\n",
      "\n",
      "Index(['tweetidg', 'tweet', 'positive', 'negative', 'neutral'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetidg</th>\n",
       "      <th>tweet</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>588687492888551424g</td>\n",
       "      <td>i am the type of nigga tryna get rich</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>592553981601304576g</td>\n",
       "      <td>sheblasiannn smuckers</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>717371522956984320g</td>\n",
       "      <td>proteinwisdom we even make japanese cars and e...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>590292541125328898g</td>\n",
       "      <td>i wanted to see my nigga when i went back home...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>592522044480225282g</td>\n",
       "      <td>wth stevo is that a nigga</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweetidg                                              tweet  \\\n",
       "0  588687492888551424g              i am the type of nigga tryna get rich   \n",
       "1  592553981601304576g                              sheblasiannn smuckers   \n",
       "2  717371522956984320g  proteinwisdom we even make japanese cars and e...   \n",
       "3  590292541125328898g  i wanted to see my nigga when i went back home...   \n",
       "4  592522044480225282g                          wth stevo is that a nigga   \n",
       "\n",
       "   positive  negative  neutral  \n",
       "0       0.0       0.0      1.0  \n",
       "1       0.0       0.0      1.0  \n",
       "2       0.0       0.0      1.0  \n",
       "3       0.0       1.0      0.0  \n",
       "4       0.0       0.0      1.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Read in csv of cleaned data\n",
    "df = pd.read_csv('df_tweets.csv', index_col=0)\n",
    "\n",
    "#Create training set\n",
    "df_train = df[df['tweet'].notnull()]\n",
    "print(df_train.shape)\n",
    "print('')\n",
    "print(df_train.columns)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages for analysis and vizualization.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages for algorithm analysis\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "#Import algorithm packages.\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a table to examine the distribution of tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweetidg', 'tweet', 'positive', 'negative', 'neutral'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Category  Number_of_Tweets    priors\n",
      "0  positive            1642.0  0.253356\n",
      "1  negative            1977.0  0.305046\n",
      "2   neutral            2862.0  0.441599\n",
      "\n",
      "Total number of labels:   6481.0\n"
     ]
    }
   ],
   "source": [
    "df_sparse = df_train.drop(['tweetidg', 'tweet'], axis=1)\n",
    "\n",
    "counts = []\n",
    "\n",
    "#Calculate the sum of tweets for each category.\n",
    "categories = list(df_sparse.columns.values)\n",
    "for cat in categories:\n",
    "    numtwts = df_sparse[cat].sum()\n",
    "    counts.append((cat, numtwts))\n",
    "\n",
    "#Create dataframe to display the category counts and priors (starting probabilities).\n",
    "df_stats = pd.DataFrame(counts, columns=['Category', 'Number_of_Tweets'])\n",
    "\n",
    "#Calculate \n",
    "total = df_stats.Number_of_Tweets.sum()\n",
    "\n",
    "#Calculate priors.\n",
    "df_stats['priors'] = df_stats['Number_of_Tweets']/total\n",
    "\n",
    "print(df_stats)\n",
    "print('')\n",
    "print('Total number of labels:  ', total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-label classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure all entries in df_matrix['text'] are strings\n",
    "df_train['tweet'] = df_train['tweet'].astype(str)\n",
    "categories = ['positive', 'negative', 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages to vectorize and split the data\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring accuracy for the MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer\n",
      "\n",
      "positive training set accuracy:  0.7654320987654321\n",
      "positive test set accuracy:  0.7552699228791774 \n",
      "\n",
      "negative training set accuracy:  0.7795414462081128\n",
      "negative test set accuracy:  0.7588688946015424 \n",
      "\n",
      "neutral training set accuracy:  0.6664462081128748\n",
      "neutral test set accuracy:  0.6627249357326478 \n",
      "\n",
      "\n",
      "TfidfVectorizer\n",
      "positive training set accuracy:  0.7766754850088183\n",
      "positive test set accuracy:  0.7660668380462725 \n",
      "\n",
      "negative training set accuracy:  0.7954144620811288\n",
      "negative test set accuracy:  0.7748071979434448 \n",
      "\n",
      "neutral training set accuracy:  0.671957671957672\n",
      "neutral test set accuracy:  0.6637532133676093 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vectorizer = CountVectorizer(stop_words='english',binary=False,max_df=.4, ngram_range=(1, 1),\n",
    "                                     min_df=1,strip_accents='unicode',max_features=200)\n",
    "\n",
    "featurizer = TfidfVectorizer(stop_words='english',binary=False,max_df=.4,min_df=1, max_features=200)\n",
    "Xv = vectorizer.fit_transform(df_train.tweet)\n",
    "Xt = featurizer.fit_transform(df_train.tweet)\n",
    "\n",
    "\n",
    "clf_NB = MultinomialNB()\n",
    "\n",
    "print(\"CountVectorizer\\n\")\n",
    "\n",
    "for category in categories:\n",
    "    y = df_train[category].values.astype(np.int)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Xv, y, test_size=0.30, random_state=42)\n",
    "\n",
    "    clf_NB.fit(X_train,y_train)\n",
    "\n",
    "    train_accuracy =clf_NB.score(X_train,y_train)\n",
    "    test_accuracy = clf_NB.score(X_test, y_test)\n",
    "    print(category, \"training set accuracy: \", train_accuracy)\n",
    "    print(category, \"test set accuracy: \", test_accuracy, '\\n')\n",
    "    \n",
    "print(\"\\nTfidfVectorizer\")\n",
    "\n",
    "for category in categories:\n",
    "    y = df_train[category].values.astype(np.int)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Xt, y, test_size=0.30, random_state=42)\n",
    "\n",
    "    clf_NB.fit(X_train,y_train)\n",
    "\n",
    "    train_accuracy =clf_NB.score(X_train,y_train)\n",
    "    test_accuracy = clf_NB.score(X_test, y_test)\n",
    "    print(category, \"training set accuracy: \", train_accuracy)\n",
    "    print(category, \"test set accuracy: \", test_accuracy, '\\n')\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is looking good! But accuracy can be deceiving. High accuracy scores can still happen if your model detects many true negatives, but few true positives. **TfidfVectorizer** performed better for accuracy with the MultinomialNB.\n",
    "\n",
    "High accuracy doesn't always mean high precision or recall. For this study, we want to decrease the probability of a Type 2 error - a false positive where we assign a label when it is not. For this, recall is a good measure to optimize because it is a measure of how correctly we assign a positive (value =  1) label to each category. It is a ratio of positive (value = 1) instances for each category that are correctly labeled by the classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune & Train the Hyperparameters for Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer\n",
      "\n",
      "  Categories             Confusion Matrix  Precision_NB  Recall_NB     F1_NB\n",
      "0   positive   [[4150, 689], [1083, 559]]      0.447917   0.340438  0.386851\n",
      "1   negative   [[3519, 985], [532, 1445]]      0.594650   0.730905  0.655775\n",
      "2    neutral  [[3010, 609], [1613, 1249]]      0.672228   0.436408  0.529237\n",
      "\n",
      "TfidfVectorizer\n",
      "  Categories             Confusion Matrix  Precision_NB  Recall_NB     F1_NB\n",
      "0   positive   [[4527, 312], [1278, 364]]      0.538462   0.221681  0.314064\n",
      "1   negative   [[3797, 707], [753, 1224]]      0.633868   0.619120  0.626407\n",
      "2    neutral  [[3031, 588], [1624, 1238]]      0.677985   0.432565  0.528157\n"
     ]
    }
   ],
   "source": [
    "\n",
    "i=1\n",
    "\n",
    "vectors = [Xv, Xt]\n",
    "for vect in vectors:\n",
    "    \n",
    "    conmatrxNB = []\n",
    "    precisionNB = []\n",
    "    recallNB = []\n",
    "    f1NB = []\n",
    "\n",
    "    clf_NB = MultinomialNB()\n",
    "\n",
    "    alphas = [.00001,.0001, .001, .01, .1, 1, 5, 10, 50]\n",
    "\n",
    "    # Create hyperparameter options\n",
    "    hyperparameters = dict(alpha=alphas)\n",
    "\n",
    "    # Create grid search using 5-fold cross validation\n",
    "    clf_tune = GridSearchCV(clf_NB, hyperparameters, cv=5, scoring='recall')               \n",
    "\n",
    "\n",
    "    for category in categories:\n",
    "        # Fit grid search\n",
    "        best_model = clf_tune.fit(vect, df_train[category])                \n",
    "        alpha = best_model.best_estimator_.get_params()['alpha']\n",
    "    \n",
    "        clf_tunedNB = MultinomialNB(alpha=alpha)\n",
    "    \n",
    "        y_train_pred = cross_val_predict(clf_tunedNB, vect, df_train[category], cv=5)\n",
    "\n",
    "    \n",
    "        cmNB = confusion_matrix(df_train[category], y_train_pred)\n",
    "        conmatrxNB.append(cmNB)\n",
    "        precNB = precision_score(df_train[category], y_train_pred)    \n",
    "        precisionNB.append(precNB)\n",
    "        recNB = recall_score(df_train[category], y_train_pred)\n",
    "        recallNB.append(recNB)\n",
    "        fNB = f1_score(df_train[category], y_train_pred)\n",
    "        f1NB.append(fNB)\n",
    "        \n",
    "\n",
    "    \n",
    "    dictNB = {'Categories':categories, 'Confusion Matrix':conmatrxNB, 'Precision_NB':precisionNB, \n",
    "          'Recall_NB':recallNB, 'F1_NB':f1NB}\n",
    "\n",
    "\n",
    "    df_NB = pd.DataFrame(dictNB) \n",
    "    #df_NB.to_csv('Results_NB_HBR.csv')\n",
    "\n",
    "    if i == 1:\n",
    "        print('CountVectorizer\\n')\n",
    "        i += 1\n",
    "    else:\n",
    "        print('\\nTfidfVectorizer')\n",
    "    \n",
    "    print(df_NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CountVectorizer gave the highest Recall scores with the tuned multinomialNB classifier. Below I will run the model again using CountVectorizer. This is to load the correct results in the Naive Bayes dataframe (df_NB). This dataframe will be used to compare the performance of the algorithms on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categories</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "      <th>Precision_NB</th>\n",
       "      <th>Recall_NB</th>\n",
       "      <th>F1_NB</th>\n",
       "      <th>auc_NB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>[[4150, 689], [1083, 559]]</td>\n",
       "      <td>0.447917</td>\n",
       "      <td>0.340438</td>\n",
       "      <td>0.386851</td>\n",
       "      <td>0.617065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>[[3519, 985], [532, 1445]]</td>\n",
       "      <td>0.594650</td>\n",
       "      <td>0.730905</td>\n",
       "      <td>0.655775</td>\n",
       "      <td>0.750151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>[[3010, 609], [1613, 1249]]</td>\n",
       "      <td>0.672228</td>\n",
       "      <td>0.436408</td>\n",
       "      <td>0.529237</td>\n",
       "      <td>0.635560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Categories             Confusion Matrix  Precision_NB  Recall_NB     F1_NB  \\\n",
       "0   positive   [[4150, 689], [1083, 559]]      0.447917   0.340438  0.386851   \n",
       "1   negative   [[3519, 985], [532, 1445]]      0.594650   0.730905  0.655775   \n",
       "2    neutral  [[3010, 609], [1613, 1249]]      0.672228   0.436408  0.529237   \n",
       "\n",
       "     auc_NB  \n",
       "0  0.617065  \n",
       "1  0.750151  \n",
       "2  0.635560  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run model with CountVectorizer and get AUC scores\n",
    "\n",
    "conmatrxNB = []\n",
    "precisionNB = []\n",
    "recallNB = []\n",
    "f1NB = []\n",
    "aucNB = []\n",
    "aveprecNB = []\n",
    "\n",
    "clf_NB = MultinomialNB()\n",
    "\n",
    "alphas = [.00001,.0001, .001, .01, .1, 1, 5, 10, 50]\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(alpha=alphas)\n",
    "\n",
    "# Create grid search using 5-fold cross validation\n",
    "clf_tune = GridSearchCV(clf_NB, hyperparameters, cv=5, scoring='recall')               \n",
    "\n",
    "\n",
    "for category in categories:\n",
    "    # Fit grid search\n",
    "    best_model = clf_tune.fit(Xv, df_train[category])                \n",
    "    alpha = best_model.best_estimator_.get_params()['alpha']\n",
    "    \n",
    "    clf_tunedNB = MultinomialNB(alpha=alpha)\n",
    "    \n",
    "    y_train_pred = cross_val_predict(clf_tunedNB, Xv, df_train[category], cv=5)\n",
    "\n",
    "    cmNB = confusion_matrix(df_train[category], y_train_pred)\n",
    "    conmatrxNB.append(cmNB)\n",
    "    precNB = precision_score(df_train[category], y_train_pred)    \n",
    "    precisionNB.append(precNB)\n",
    "    recNB = recall_score(df_train[category], y_train_pred)\n",
    "    recallNB.append(recNB)\n",
    "    fNB = f1_score(df_train[category], y_train_pred)\n",
    "    f1NB.append(fNB)\n",
    "    \n",
    "    #Make training and testing data.\n",
    "    y = df_train[category].values.astype(np.int)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(Xv, y, test_size=0.3)\n",
    "    \n",
    "    #Fit the tuned classifier to the training data.\n",
    "    clf_tunedNB.fit(X_train, y_train)\n",
    "\n",
    "    #Calculate AUC score with the test set\n",
    "    y_pred = clf_tunedNB.predict(X_test)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "    auc_nb = roc_auc_score(y_test, y_pred)\n",
    "    aucNB.append(auc_nb)\n",
    "    \n",
    "\n",
    "dictNB = {'Categories':categories, 'Confusion Matrix':conmatrxNB, 'Precision_NB':precisionNB, \n",
    "          'Recall_NB':recallNB, 'F1_NB':f1NB, 'auc_NB':aucNB}\n",
    "\n",
    "\n",
    "df_NB = pd.DataFrame(dictNB) \n",
    "#df_NB.to_csv('Results_NB_HBR.csv')\n",
    "    \n",
    "df_NB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune & Train the hyperparameters for LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer\n",
      "\n",
      "  Categories             Confusion Matrix  Precision_SVC  Recall_SVC    F1_SVC\n",
      "0   positive   [[3281, 1558], [869, 773]]       0.331617    0.470767  0.389127\n",
      "1   negative  [[3111, 1393], [692, 1285]]       0.479836    0.649975  0.552095\n",
      "2    neutral  [[1761, 1858], [928, 1934]]       0.510021    0.675751  0.581304\n",
      "\n",
      "TfidfVectorizer\n",
      "  Categories              Confusion Matrix  Precision_SVC  Recall_SVC  \\\n",
      "0   positive    [[3153, 1686], [865, 777]]       0.315469    0.473203   \n",
      "1   negative   [[3075, 1429], [788, 1189]]       0.454163    0.601416   \n",
      "2    neutral  [[1992, 1627], [1012, 1850]]       0.532068    0.646401   \n",
      "\n",
      "     F1_SVC  \n",
      "0  0.378563  \n",
      "1  0.517519  \n",
      "2  0.583688  \n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "i=1\n",
    "\n",
    "vectors = [Xv, Xt]\n",
    "for vect in vectors:\n",
    "\n",
    "    conmatrxSVC = []\n",
    "    precisionSVC = []\n",
    "    recallSVC = []\n",
    "    f1SVC = []\n",
    "    \n",
    "    clf_SVC = LinearSVC(random_state=42)\n",
    "\n",
    "    loss = ['hinge', 'squared_hinge']\n",
    "    \n",
    "    C = [1, 10, 100, 1000, 10000]\n",
    "\n",
    "    # Create hyperparameter options\n",
    "    hyperparameters = dict(C=C, loss=loss)\n",
    "\n",
    "    # Create grid search using 5-fold cross validation\n",
    "    clf_tune = GridSearchCV(clf_SVC, hyperparameters, cv=5, scoring='recall')   \n",
    "\n",
    "    for category in categories:\n",
    "        # Fit grid search\n",
    "        best_model = clf_tune.fit(vect, df_train[category])                \n",
    "    \n",
    "        SVCloss = best_model.best_estimator_.get_params()['loss']\n",
    "        SVC_C = best_model.best_estimator_.get_params()['C']\n",
    "    \n",
    "        clf_tunedSVC = LinearSVC(random_state=42, loss=SVCloss, C=SVC_C)\n",
    "    \n",
    "        y_train_pred = cross_val_predict(clf_tunedSVC, vect, df_train[category], cv=5)\n",
    "   \n",
    "        cmSVC = confusion_matrix(df_train[category], y_train_pred)\n",
    "        conmatrxSVC.append(cmSVC)\n",
    "        precSVC = precision_score(df_train[category], y_train_pred)    \n",
    "        precisionSVC.append(precSVC)\n",
    "        recSVC = recall_score(df_train[category], y_train_pred)\n",
    "        recallSVC.append(recSVC)\n",
    "        fSVC = f1_score(df_train[category], y_train_pred)\n",
    "        f1SVC.append(fSVC)\n",
    "    \n",
    "    dictSVC = {'Categories':categories, 'Confusion Matrix':conmatrxSVC, 'Precision_SVC':precisionSVC, \n",
    "           'Recall_SVC':recallSVC, 'F1_SVC':f1SVC}\n",
    "    df_SVC = pd.DataFrame(dictSVC) \n",
    "    #df_SVC.to_csv('Results_SVC_HBR.csv')\n",
    "\n",
    "    if i == 1:\n",
    "        print('CountVectorizer\\n')\n",
    "        i += 1\n",
    "    else:\n",
    "        print('\\nTfidfVectorizer')\n",
    "    \n",
    "    print(df_SVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer gave slightly better recall scores for LinearSVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categories</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "      <th>Precision_SVC</th>\n",
       "      <th>Recall_SVC</th>\n",
       "      <th>F1_SVC</th>\n",
       "      <th>auc_SVC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>[[3281, 1558], [869, 773]]</td>\n",
       "      <td>0.331617</td>\n",
       "      <td>0.470767</td>\n",
       "      <td>0.389127</td>\n",
       "      <td>0.558591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>[[3111, 1393], [692, 1285]]</td>\n",
       "      <td>0.479836</td>\n",
       "      <td>0.649975</td>\n",
       "      <td>0.552095</td>\n",
       "      <td>0.659201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>[[1761, 1858], [928, 1934]]</td>\n",
       "      <td>0.510021</td>\n",
       "      <td>0.675751</td>\n",
       "      <td>0.581304</td>\n",
       "      <td>0.616927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Categories             Confusion Matrix  Precision_SVC  Recall_SVC  \\\n",
       "0   positive   [[3281, 1558], [869, 773]]       0.331617    0.470767   \n",
       "1   negative  [[3111, 1393], [692, 1285]]       0.479836    0.649975   \n",
       "2    neutral  [[1761, 1858], [928, 1934]]       0.510021    0.675751   \n",
       "\n",
       "     F1_SVC   auc_SVC  \n",
       "0  0.389127  0.558591  \n",
       "1  0.552095  0.659201  \n",
       "2  0.581304  0.616927  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run model with CountVectorizer and get AUC scores\n",
    "\n",
    "conmatrxSVC = []\n",
    "precisionSVC = []\n",
    "recallSVC = []\n",
    "f1SVC = []\n",
    "aucSVC = []\n",
    "\n",
    "clf_SVC = LinearSVC(random_state=42)\n",
    "\n",
    "loss = ['hinge', 'squared_hinge']\n",
    "    \n",
    "C = [1, 10, 100, 1000, 10000]\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C, loss=loss)\n",
    "\n",
    "# Create grid search using 5-fold cross validation\n",
    "clf_tune = GridSearchCV(clf_SVC, hyperparameters, cv=5, scoring='recall')   \n",
    "\n",
    "\n",
    "for category in categories:\n",
    "    \n",
    "    # Fit grid search\n",
    "    best_model = clf_tune.fit(Xv, df_train[category])                \n",
    "    \n",
    "    SVCloss = best_model.best_estimator_.get_params()['loss']\n",
    "    SVC_C = best_model.best_estimator_.get_params()['C']\n",
    "    \n",
    "    clf_tunedSVC = LinearSVC(random_state=42, loss=SVCloss, C=SVC_C)\n",
    "    \n",
    "    y_train_pred = cross_val_predict(clf_tunedSVC, Xv, df_train[category], cv=5)\n",
    "   \n",
    "    #Get algorithm performance measures\n",
    "    cmSVC = confusion_matrix(df_train[category], y_train_pred)\n",
    "    conmatrxSVC.append(cmSVC)\n",
    "    precSVC = precision_score(df_train[category], y_train_pred)    \n",
    "    precisionSVC.append(precSVC)\n",
    "    recSVC = recall_score(df_train[category], y_train_pred)\n",
    "    recallSVC.append(recSVC)\n",
    "    fSVC = f1_score(df_train[category], y_train_pred)\n",
    "    f1SVC.append(fSVC)\n",
    "    \n",
    "    #Make training and testing data.\n",
    "    y = df_train[category].values.astype(np.int)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(Xv, y, test_size=0.3)\n",
    "    \n",
    "    #Fit the tuned classifier to the training data.\n",
    "    clf_tunedSVC.fit(X_train, y_train)\n",
    "\n",
    "    #Calculate AUC score with the test set\n",
    "    y_pred = clf_tunedSVC.predict(X_test)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "    auc_svc = roc_auc_score(y_test, y_pred)\n",
    "    aucSVC.append(auc_svc)\n",
    "    \n",
    "dictSVC = {'Categories':categories, 'Confusion Matrix':conmatrxSVC, 'Precision_SVC':precisionSVC, \n",
    "           'Recall_SVC':recallSVC, 'F1_SVC':f1SVC, 'auc_SVC':aucSVC}\n",
    "df_SVC = pd.DataFrame(dictSVC) \n",
    "#df_SVC.to_csv('Results_SVC_HBR.csv')\n",
    "\n",
    "df_SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune & Train Hyperparameters for LR\n",
    "\n",
    "All solvers support the l2 penalty (‘newton-cg’, ‘sag’ and ‘lbfgs’ solvers do not support L1 penalty). So, hyperparameters are optimized using the L2 penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer\n",
      "\n",
      "  Categories             Confusion Matrix  Precision_LR  Recall_LR     F1_LR\n",
      "0   positive   [[4464, 375], [1241, 401]]      0.516753   0.244214  0.331679\n",
      "1   negative   [[4051, 453], [957, 1020]]      0.692464   0.515933  0.591304\n",
      "2    neutral  [[2730, 889], [1169, 1693]]      0.655693   0.591544  0.621969\n",
      "\n",
      "TfidfVectorizer\n",
      "  Categories             Confusion Matrix  Precision_LR  Recall_LR     F1_LR\n",
      "0   positive   [[4447, 392], [1235, 407]]      0.509387   0.247868  0.333470\n",
      "1   negative   [[3935, 569], [834, 1143]]      0.667640   0.578149  0.619680\n",
      "2    neutral  [[2786, 833], [1235, 1627]]      0.661382   0.568484  0.611424\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "\n",
    "vectors = [Xv, Xt]\n",
    "for vect in vectors:\n",
    "\n",
    "    conmatrxLR = []\n",
    "    precisionLR = []\n",
    "    recallLR = []\n",
    "    f1LR = []\n",
    "\n",
    "    clf_LR = LogisticRegression(random_state=42)\n",
    "    \n",
    "    # Create regularization hyperparameter space\n",
    "    C = np.logspace(-4, 4, 20)\n",
    "\n",
    "    solver = ['liblinear', 'sag', 'newton-cg', 'saga', 'lbfgs']\n",
    "\n",
    "    # Create hyperparameter options\n",
    "    hyperparameters = dict(solver = solver, C=C)\n",
    "\n",
    "    # Create grid search using 5-fold cross validation\n",
    "    clf_tuneLR = GridSearchCV(clf_LR, hyperparameters, cv=5, verbose=0, scoring='recall')       \n",
    "\n",
    "    for category in categories:\n",
    "    \n",
    "        # Fit grid search\n",
    "        best_model = clf_tuneLR.fit(vect, df_train[category])                \n",
    "        LRsolver = best_model.best_estimator_.get_params()['solver']\n",
    "        LR_C = best_model.best_estimator_.get_params()['C']      \n",
    "\n",
    "        clf_tunedLR = LogisticRegression(penalty='l2', C=LR_C, solver=LRsolver)\n",
    "    \n",
    "        y_train_pred = cross_val_predict(clf_tunedLR, vect, df_train[category], cv=5)\n",
    "    \n",
    "        cmLR = confusion_matrix(df_train[category], y_train_pred)\n",
    "        conmatrxLR.append(cmLR)\n",
    "        precLR = precision_score(df_train[category], y_train_pred)    \n",
    "        precisionLR.append(precLR)\n",
    "        recLR = recall_score(df_train[category], y_train_pred)\n",
    "        recallLR.append(recLR)\n",
    "        fLR = f1_score(df_train[category], y_train_pred)\n",
    "        f1LR.append(fLR)\n",
    "        \n",
    "        \n",
    "\n",
    "    dictLR = {'Categories':categories, 'Confusion Matrix':conmatrxLR, 'Precision_LR':precisionLR, \n",
    "           'Recall_LR':recallLR, 'F1_LR':f1LR}\n",
    "    df_LR = pd.DataFrame(dictLR) \n",
    "    #df_LR.to_csv('Results_LR_HBR.csv')\n",
    " \n",
    "    if i == 1:\n",
    "        print('CountVectorizer\\n')\n",
    "        i += 1\n",
    "    else:\n",
    "        print('\\nTfidfVectorizer')\n",
    "    \n",
    "    print(df_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 out of 3 labels performed better with TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categories</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "      <th>Precision_LR</th>\n",
       "      <th>Recall_LR</th>\n",
       "      <th>F1_LR</th>\n",
       "      <th>auc_LR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>[[4447, 392], [1235, 407]]</td>\n",
       "      <td>0.509387</td>\n",
       "      <td>0.247868</td>\n",
       "      <td>0.333470</td>\n",
       "      <td>0.578982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>[[3935, 569], [834, 1143]]</td>\n",
       "      <td>0.667640</td>\n",
       "      <td>0.578149</td>\n",
       "      <td>0.619680</td>\n",
       "      <td>0.718866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>[[2786, 833], [1235, 1627]]</td>\n",
       "      <td>0.661382</td>\n",
       "      <td>0.568484</td>\n",
       "      <td>0.611424</td>\n",
       "      <td>0.672090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Categories             Confusion Matrix  Precision_LR  Recall_LR     F1_LR  \\\n",
       "0   positive   [[4447, 392], [1235, 407]]      0.509387   0.247868  0.333470   \n",
       "1   negative   [[3935, 569], [834, 1143]]      0.667640   0.578149  0.619680   \n",
       "2    neutral  [[2786, 833], [1235, 1627]]      0.661382   0.568484  0.611424   \n",
       "\n",
       "     auc_LR  \n",
       "0  0.578982  \n",
       "1  0.718866  \n",
       "2  0.672090  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run model with CountVectorizer and get AUC scores\n",
    "\n",
    "conmatrxLR = []\n",
    "precisionLR = []\n",
    "recallLR = []\n",
    "f1LR = []\n",
    "aucLR = []\n",
    "\n",
    "# Create regularization hyperparameter space\n",
    "C = np.logspace(-4, 4, 20)\n",
    "\n",
    "solver = ['liblinear', 'sag', 'newton-cg', 'saga', 'lbfgs']\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(solver = solver, C=C)\n",
    "\n",
    "# Create grid search using 5-fold cross validation\n",
    "clf_tuneLR = GridSearchCV(clf_LR, hyperparameters, cv=5, verbose=0, scoring='recall')       \n",
    "\n",
    "for category in categories:\n",
    "    \n",
    "    # Fit grid search\n",
    "    best_model = clf_tuneLR.fit(Xt, df_train[category])                \n",
    "    LRsolver = best_model.best_estimator_.get_params()['solver']\n",
    "    LR_C = best_model.best_estimator_.get_params()['C']      \n",
    "\n",
    "    clf_tunedLR = LogisticRegression(penalty='l2', C=LR_C, solver=LRsolver)\n",
    "    \n",
    "    y_train_pred = cross_val_predict(clf_tunedLR, Xt, df_train[category], cv=5)\n",
    "    \n",
    "    cmLR = confusion_matrix(df_train[category], y_train_pred)\n",
    "    conmatrxLR.append(cmLR)\n",
    "    precLR = precision_score(df_train[category], y_train_pred)    \n",
    "    precisionLR.append(precLR)\n",
    "    recLR = recall_score(df_train[category], y_train_pred)\n",
    "    recallLR.append(recLR)\n",
    "    fLR = f1_score(df_train[category], y_train_pred)\n",
    "    f1LR.append(fLR)\n",
    "    \n",
    "    #Make training and testing data.\n",
    "    y = df_train[category].values.astype(np.int)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(Xt, y, test_size=0.3)\n",
    "    \n",
    "    #Fit the tuned classifier to the training data.\n",
    "    clf_tunedLR.fit(X_train, y_train)\n",
    "\n",
    "    #Calculate AUC score with the test set\n",
    "    y_pred = clf_tunedLR.predict(X_test)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "    auc_lr = roc_auc_score(y_test, y_pred)\n",
    "    aucLR.append(auc_lr)\n",
    "\n",
    "dictLR = {'Categories':categories, 'Confusion Matrix':conmatrxLR, 'Precision_LR':precisionLR, \n",
    "           'Recall_LR':recallLR, 'F1_LR':f1LR, 'auc_LR':aucLR}\n",
    "df_LR = pd.DataFrame(dictLR) \n",
    "#df_LR.to_csv('Results_LR.csv')\n",
    "\n",
    "df_LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune & Train Hyperparameters for RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer\n",
      "\n",
      "  Categories              Confusion Matrix  Precision_RF  Recall_RF     F1_RF\n",
      "0   positive    [[4058, 781], [1151, 491]]      0.386006   0.299026  0.336994\n",
      "1   negative    [[3801, 703], [841, 1136]]      0.617727   0.574608  0.595388\n",
      "2    neutral  [[2475, 1144], [1136, 1726]]      0.601394   0.603075  0.602233\n",
      "\n",
      "TfidfVectorizer\n",
      "  Categories              Confusion Matrix  Precision_RF  Recall_RF     F1_RF\n",
      "0   positive    [[4061, 778], [1136, 506]]      0.394081   0.308161  0.345865\n",
      "1   negative    [[3834, 670], [843, 1134]]      0.628603   0.573596  0.599841\n",
      "2    neutral  [[2584, 1035], [1249, 1613]]      0.609139   0.563592  0.585481\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "\n",
    "vectors = [Xv, Xt]\n",
    "for vect in vectors:\n",
    "\n",
    "    conmatrxRF = []\n",
    "    precisionRF = []\n",
    "    recallRF = []\n",
    "    f1RF = []\n",
    "\n",
    "    clf_RF = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    n_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200]\n",
    "\n",
    "    # Create hyperparameter options\n",
    "    hyperparameters = dict(n_estimators=n_estimators)\n",
    "\n",
    "\n",
    "    # Create grid search using 5-fold cross validation\n",
    "    clf_tune = GridSearchCV(clf_RF, hyperparameters, cv=5, scoring='recall')\n",
    "\n",
    "    for category in categories:\n",
    "        # Fit grid search\n",
    "        best_model = clf_tune.fit(vect, df_train[category]) \n",
    "        RFestimators = best_model.best_estimator_.get_params()['n_estimators']\n",
    "                \n",
    "        clf_tunedRF = RandomForestClassifier(random_state=42, n_estimators=RFestimators)\n",
    "    \n",
    "        y_train_pred = cross_val_predict(clf_tunedRF, vect, df_train[category], cv=5)\n",
    "\n",
    "    \n",
    "        cmRF = confusion_matrix(df_train[category], y_train_pred)\n",
    "        conmatrxRF.append(cmRF)\n",
    "        precRF = precision_score(df_train[category], y_train_pred)    \n",
    "        precisionRF.append(precRF)\n",
    "        recRF = recall_score(df_train[category], y_train_pred)\n",
    "        recallRF.append(recRF)\n",
    "        fRF = f1_score(df_train[category], y_train_pred)\n",
    "        f1RF.append(fRF)\n",
    "\n",
    "    \n",
    "    dictRF = {'Categories':categories, 'Confusion Matrix':conmatrxRF, 'Precision_RF':precisionRF, \n",
    "           'Recall_RF':recallRF, 'F1_RF':f1RF}\n",
    "    df_RF = pd.DataFrame(dictRF) \n",
    "    #df_RF.to_csv('Results_RF.csv')\n",
    "\n",
    "    if i == 1:\n",
    "        print('CountVectorizer\\n')\n",
    "        i += 1\n",
    "    else:\n",
    "        print('\\nTfidfVectorizer')\n",
    "    \n",
    "    print(df_RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer gave slightly better results for RandomForestClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categories</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "      <th>Precision_RF</th>\n",
       "      <th>Recall_RF</th>\n",
       "      <th>F1_RF</th>\n",
       "      <th>auc_RF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>[[4058, 781], [1151, 491]]</td>\n",
       "      <td>0.386006</td>\n",
       "      <td>0.299026</td>\n",
       "      <td>0.336994</td>\n",
       "      <td>0.570422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>[[3801, 703], [841, 1136]]</td>\n",
       "      <td>0.617727</td>\n",
       "      <td>0.574608</td>\n",
       "      <td>0.595388</td>\n",
       "      <td>0.713677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>[[2475, 1144], [1136, 1726]]</td>\n",
       "      <td>0.601394</td>\n",
       "      <td>0.603075</td>\n",
       "      <td>0.602233</td>\n",
       "      <td>0.629962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Categories              Confusion Matrix  Precision_RF  Recall_RF     F1_RF  \\\n",
       "0   positive    [[4058, 781], [1151, 491]]      0.386006   0.299026  0.336994   \n",
       "1   negative    [[3801, 703], [841, 1136]]      0.617727   0.574608  0.595388   \n",
       "2    neutral  [[2475, 1144], [1136, 1726]]      0.601394   0.603075  0.602233   \n",
       "\n",
       "     auc_RF  \n",
       "0  0.570422  \n",
       "1  0.713677  \n",
       "2  0.629962  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run model with CountVectorizer and get AUC scores\n",
    "\n",
    "conmatrxRF = []\n",
    "precisionRF = []\n",
    "recallRF = []\n",
    "f1RF = []\n",
    "aucRF = []\n",
    "\n",
    "clf_RF = RandomForestClassifier(random_state=42)\n",
    "\n",
    "n_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200]\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(n_estimators=n_estimators)\n",
    "\n",
    "\n",
    "# Create grid search using 5-fold cross validation\n",
    "clf_tune = GridSearchCV(clf_RF, hyperparameters, cv=5, scoring='recall')\n",
    "\n",
    "for category in categories:\n",
    "    # Fit grid search\n",
    "    best_model = clf_tune.fit(Xv, df_train[category]) \n",
    "    RFestimators = best_model.best_estimator_.get_params()['n_estimators']\n",
    "                \n",
    "    clf_tunedRF = RandomForestClassifier(random_state=42, n_estimators=RFestimators)\n",
    "    \n",
    "    y_train_pred = cross_val_predict(clf_tunedRF, Xv, df_train[category], cv=5)\n",
    "\n",
    "    \n",
    "    cmRF = confusion_matrix(df_train[category], y_train_pred)\n",
    "    conmatrxRF.append(cmRF)\n",
    "    precRF = precision_score(df_train[category], y_train_pred)    \n",
    "    precisionRF.append(precRF)\n",
    "    recRF = recall_score(df_train[category], y_train_pred)\n",
    "    recallRF.append(recRF)\n",
    "    fRF = f1_score(df_train[category], y_train_pred)\n",
    "    f1RF.append(fRF)\n",
    "    \n",
    "    #Make training and testing data.\n",
    "    y = df_train[category].values.astype(np.int)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(Xv, y, test_size=0.3)\n",
    "    \n",
    "    #Fit the tuned classifier to the training data.\n",
    "    clf_tunedRF.fit(X_train, y_train)\n",
    "\n",
    "    #Calculate AUC score with the test set\n",
    "    y_pred = clf_tunedRF.predict(X_test)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "    auc_rf = roc_auc_score(y_test, y_pred)\n",
    "    aucRF.append(auc_rf)\n",
    "\n",
    "    \n",
    "dictRF = {'Categories':categories, 'Confusion Matrix':conmatrxRF, 'Precision_RF':precisionRF, \n",
    "           'Recall_RF':recallRF, 'F1_RF':f1RF, 'auc_RF':aucRF}\n",
    "df_RF = pd.DataFrame(dictRF) \n",
    "#df_RF.to_csv('Results_RF_HBR.csv')\n",
    "\n",
    "df_RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune & Train Hyperparameters for Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer\n",
      "\n",
      "  Categories             Confusion Matrix  Precision_GRD  Recall_GRD    F1_GRD\n",
      "0   positive   [[4559, 280], [1329, 313]]       0.527825    0.190621  0.280089\n",
      "1   negative   [[4088, 416], [1018, 959]]       0.697455    0.485078  0.572196\n",
      "2    neutral  [[2321, 1298], [941, 1921]]       0.596769    0.671209  0.631804\n",
      "\n",
      "TfidfVectorizer\n",
      "  Categories             Confusion Matrix  Precision_GRD  Recall_GRD    F1_GRD\n",
      "0   positive   [[4610, 229], [1328, 314]]       0.578269    0.191230  0.287414\n",
      "1   negative   [[4049, 455], [1000, 977]]       0.682263    0.494183  0.573189\n",
      "2    neutral  [[2246, 1373], [923, 1939]]       0.585447    0.677498  0.628118\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "\n",
    "vectors = [Xv, Xt]\n",
    "for vect in vectors:\n",
    "\n",
    "    conmatrxGRD = []\n",
    "    precisionGRD = []\n",
    "    recallGRD = []\n",
    "    f1GRD = []\n",
    "\n",
    "    clf_GRD = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "    loss=['deviance', 'exponential']\n",
    "    n_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200]\n",
    "\n",
    "\n",
    "    # Create hyperparameter options\n",
    "    hyperparameters = dict(loss=loss, n_estimators=n_estimators)\n",
    "\n",
    "\n",
    "    # Create grid search using 5-fold cross validation\n",
    "    clf_tune = GridSearchCV(clf_GRD, hyperparameters, cv=5, scoring='recall')\n",
    "\n",
    "    for category in categories:\n",
    "\n",
    "        # Fit grid search\n",
    "        best_model = clf_tune.fit(vect, df_train[category]) \n",
    "        GRDloss = best_model.best_estimator_.get_params()['loss']\n",
    "        GRDestimators = best_model.best_estimator_.get_params()['n_estimators']\n",
    "                 \n",
    " \n",
    "        clf_tunedGRD= GradientBoostingClassifier(random_state=42, loss=GRDloss, n_estimators=GRDestimators)\n",
    "    \n",
    "        y_train_pred = cross_val_predict(clf_tunedGRD, vect, df_train[category], cv=5)\n",
    "    \n",
    "        cmGRD = confusion_matrix(df_train[category], y_train_pred)\n",
    "        conmatrxGRD.append(cmGRD)\n",
    "        precGRD = precision_score(df_train[category], y_train_pred)    \n",
    "        precisionGRD.append(precGRD)\n",
    "        recGRD = recall_score(df_train[category], y_train_pred)\n",
    "        recallGRD.append(recGRD)\n",
    "        fGRD = f1_score(df_train[category], y_train_pred)\n",
    "        f1GRD.append(fGRD)\n",
    "\n",
    "    \n",
    "    dictGRD = {'Categories':categories, 'Confusion Matrix':conmatrxGRD, 'Precision_GRD':precisionGRD, \n",
    "           'Recall_GRD':recallGRD, 'F1_GRD':f1GRD}\n",
    "    df_GRD = pd.DataFrame(dictGRD) \n",
    "    #df_GRDt.to_csv('Results_GRD.csv')\n",
    "\n",
    "    if i == 1:\n",
    "        print('CountVectorizer\\n')\n",
    "        i += 1\n",
    "    else:\n",
    "        print('\\nTfidfVectorizer')\n",
    "    \n",
    "    print(df_GRD)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best recall is with TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categories</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "      <th>Precision_GRD</th>\n",
       "      <th>Recall_GRD</th>\n",
       "      <th>F1_GRD</th>\n",
       "      <th>auc_GRD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>[[4610, 229], [1328, 314]]</td>\n",
       "      <td>0.578269</td>\n",
       "      <td>0.191230</td>\n",
       "      <td>0.287414</td>\n",
       "      <td>0.571353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>[[4049, 455], [1000, 977]]</td>\n",
       "      <td>0.682263</td>\n",
       "      <td>0.494183</td>\n",
       "      <td>0.573189</td>\n",
       "      <td>0.699363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>[[2246, 1373], [923, 1939]]</td>\n",
       "      <td>0.585447</td>\n",
       "      <td>0.677498</td>\n",
       "      <td>0.628118</td>\n",
       "      <td>0.653992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Categories             Confusion Matrix  Precision_GRD  Recall_GRD  \\\n",
       "0   positive   [[4610, 229], [1328, 314]]       0.578269    0.191230   \n",
       "1   negative   [[4049, 455], [1000, 977]]       0.682263    0.494183   \n",
       "2    neutral  [[2246, 1373], [923, 1939]]       0.585447    0.677498   \n",
       "\n",
       "     F1_GRD   auc_GRD  \n",
       "0  0.287414  0.571353  \n",
       "1  0.573189  0.699363  \n",
       "2  0.628118  0.653992  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conmatrxGRD = []\n",
    "precisionGRD = []\n",
    "recallGRD = []\n",
    "f1GRD = []\n",
    "aucGRD = []\n",
    "\n",
    "clf_GRD = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "loss=['deviance', 'exponential']\n",
    "n_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200]\n",
    "\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(loss=loss, n_estimators=n_estimators)\n",
    "\n",
    "\n",
    "# Create grid search using 5-fold cross validation\n",
    "clf_tune = GridSearchCV(clf_GRD, hyperparameters, cv=5, scoring='recall')\n",
    "\n",
    "for category in categories:\n",
    "\n",
    "    # Fit grid search\n",
    "    best_model = clf_tune.fit(Xt, df_train[category]) \n",
    "    GRDloss = best_model.best_estimator_.get_params()['loss']\n",
    "    GRDestimators = best_model.best_estimator_.get_params()['n_estimators']\n",
    "                 \n",
    " \n",
    "    clf_tunedGRD= GradientBoostingClassifier(random_state=42, loss=GRDloss, n_estimators=GRDestimators)\n",
    "    \n",
    "    y_train_pred = cross_val_predict(clf_tunedGRD, Xt, df_train[category], cv=5)\n",
    "    \n",
    "    cmGRD = confusion_matrix(df_train[category], y_train_pred)\n",
    "    conmatrxGRD.append(cmGRD)\n",
    "    precGRD = precision_score(df_train[category], y_train_pred)    \n",
    "    precisionGRD.append(precGRD)\n",
    "    recGRD = recall_score(df_train[category], y_train_pred)\n",
    "    recallGRD.append(recGRD)\n",
    "    fGRD = f1_score(df_train[category], y_train_pred)\n",
    "    f1GRD.append(fGRD)\n",
    "\n",
    "    \n",
    "    #Make training and testing data.\n",
    "    y = df_train[category].values.astype(np.int)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(Xt, y, test_size=0.3)\n",
    "    \n",
    "    #Fit the tuned classifier to the training data.\n",
    "    clf_tunedGRD.fit(X_train, y_train)\n",
    "\n",
    "    #Calculate AUC score with the test set\n",
    "    y_pred = clf_tunedGRD.predict(X_test)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "    auc_grd = roc_auc_score(y_test, y_pred)\n",
    "    aucGRD.append(auc_grd)\n",
    "\n",
    "    \n",
    "dictGRD = {'Categories':categories, 'Confusion Matrix':conmatrxGRD, 'Precision_GRD':precisionGRD, \n",
    "           'Recall_GRD':recallGRD, 'F1_GRD':f1GRD, 'auc_GRD':aucGRD}\n",
    "df_GRD = pd.DataFrame(dictGRD) \n",
    "#df_GRD.to_csv('Results_GRD_HBR.csv')\n",
    "\n",
    "df_GRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categories</th>\n",
       "      <th>Precision_NB</th>\n",
       "      <th>Precision_SVC</th>\n",
       "      <th>Precision_LR</th>\n",
       "      <th>Precision_RF</th>\n",
       "      <th>Precision_GRD</th>\n",
       "      <th>Max_Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>0.447917</td>\n",
       "      <td>0.331617</td>\n",
       "      <td>0.509387</td>\n",
       "      <td>0.386006</td>\n",
       "      <td>0.578269</td>\n",
       "      <td>0.578269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>0.594650</td>\n",
       "      <td>0.479836</td>\n",
       "      <td>0.667640</td>\n",
       "      <td>0.617727</td>\n",
       "      <td>0.682263</td>\n",
       "      <td>0.682263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0.672228</td>\n",
       "      <td>0.510021</td>\n",
       "      <td>0.661382</td>\n",
       "      <td>0.601394</td>\n",
       "      <td>0.585447</td>\n",
       "      <td>0.672228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Categories  Precision_NB  Precision_SVC  Precision_LR  Precision_RF  \\\n",
       "0   positive      0.447917       0.331617      0.509387      0.386006   \n",
       "1   negative      0.594650       0.479836      0.667640      0.617727   \n",
       "2    neutral      0.672228       0.510021      0.661382      0.601394   \n",
       "\n",
       "   Precision_GRD  Max_Precision  \n",
       "0       0.578269       0.578269  \n",
       "1       0.682263       0.682263  \n",
       "2       0.585447       0.672228  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prec1 = pd.DataFrame(columns = ['Categories', 'Precision_NB', 'Precision_SVC', \n",
    "                                   'Precision_LR', 'Precision_RF', 'Precision_GRD', 'Max_Precision'])\n",
    "df_prec1.Categories = df_NB.Categories\n",
    "df_prec1.Precision_NB = df_NB.Precision_NB\n",
    "df_prec1.Precision_SVC  = df_SVC.Precision_SVC\n",
    "df_prec1.Precision_LR  = df_LR.Precision_LR\n",
    "df_prec1.Precision_RF = df_RF.Precision_RF\n",
    "df_prec1.Precision_GRD = df_GRD.Precision_GRD\n",
    "\n",
    "\n",
    "df_prec1.Max_Precision = df_prec1.max(axis=1)\n",
    "\n",
    "#df_prec1.to_csv('/Users/valery/Documents/Springboard/Capstone_2/Pretuned_Results/Pretuned_Precision.csv')\n",
    "\n",
    "df_prec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categories</th>\n",
       "      <th>Recall_NB</th>\n",
       "      <th>Recall_SVC</th>\n",
       "      <th>Recall_LR</th>\n",
       "      <th>Recall_RF</th>\n",
       "      <th>Recall_GRD</th>\n",
       "      <th>Max_Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>0.340438</td>\n",
       "      <td>0.470767</td>\n",
       "      <td>0.247868</td>\n",
       "      <td>0.299026</td>\n",
       "      <td>0.191230</td>\n",
       "      <td>0.470767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>0.730905</td>\n",
       "      <td>0.649975</td>\n",
       "      <td>0.578149</td>\n",
       "      <td>0.574608</td>\n",
       "      <td>0.494183</td>\n",
       "      <td>0.730905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0.436408</td>\n",
       "      <td>0.675751</td>\n",
       "      <td>0.568484</td>\n",
       "      <td>0.603075</td>\n",
       "      <td>0.677498</td>\n",
       "      <td>0.677498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Categories  Recall_NB  Recall_SVC  Recall_LR  Recall_RF  Recall_GRD  \\\n",
       "0   positive   0.340438    0.470767   0.247868   0.299026    0.191230   \n",
       "1   negative   0.730905    0.649975   0.578149   0.574608    0.494183   \n",
       "2    neutral   0.436408    0.675751   0.568484   0.603075    0.677498   \n",
       "\n",
       "   Max_Recall  \n",
       "0    0.470767  \n",
       "1    0.730905  \n",
       "2    0.677498  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rec = pd.DataFrame(columns = ['Categories', 'Recall_NB', 'Recall_SVC', \n",
    "                                   'Recall_LR', 'Recall_RF', 'Recall_GRD','Max_Recall'])\n",
    "df_rec.Categories = df_NB.Categories\n",
    "df_rec.Recall_NB = df_NB.Recall_NB\n",
    "df_rec.Recall_SVC  = df_SVC.Recall_SVC\n",
    "df_rec.Recall_LR  = df_LR.Recall_LR\n",
    "df_rec.Recall_RF = df_RF.Recall_RF\n",
    "df_rec.Recall_GRD = df_GRD.Recall_GRD\n",
    "df_rec.Max_Recall = df_rec.max(axis=1)\n",
    "\n",
    "#df_rec.to_csv('/Users/valery/Documents/Springboard/Capstone_2/Pretuned_Results/Pretuned_Recall.csv')\n",
    "\n",
    "df_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categories</th>\n",
       "      <th>F1_NB</th>\n",
       "      <th>F1_SVC</th>\n",
       "      <th>F1_LR</th>\n",
       "      <th>F1_RF</th>\n",
       "      <th>F1_GRD</th>\n",
       "      <th>Max_F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>0.386851</td>\n",
       "      <td>0.389127</td>\n",
       "      <td>0.333470</td>\n",
       "      <td>0.336994</td>\n",
       "      <td>0.287414</td>\n",
       "      <td>0.389127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>0.655775</td>\n",
       "      <td>0.552095</td>\n",
       "      <td>0.619680</td>\n",
       "      <td>0.595388</td>\n",
       "      <td>0.573189</td>\n",
       "      <td>0.655775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0.529237</td>\n",
       "      <td>0.581304</td>\n",
       "      <td>0.611424</td>\n",
       "      <td>0.602233</td>\n",
       "      <td>0.628118</td>\n",
       "      <td>0.628118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Categories     F1_NB    F1_SVC     F1_LR     F1_RF    F1_GRD    Max_F1\n",
       "0   positive  0.386851  0.389127  0.333470  0.336994  0.287414  0.389127\n",
       "1   negative  0.655775  0.552095  0.619680  0.595388  0.573189  0.655775\n",
       "2    neutral  0.529237  0.581304  0.611424  0.602233  0.628118  0.628118"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_f1 = pd.DataFrame(columns = ['Categories', 'F1_NB', 'F1_SVC', \n",
    "                                   'F1_LR', 'F1_RF','F1_GRD','Max_F1'])\n",
    "df_f1.Categories = df_NB.Categories\n",
    "df_f1.F1_NB = df_NB.F1_NB\n",
    "df_f1.F1_SVC  = df_SVC.F1_SVC\n",
    "df_f1.F1_LR  = df_LR.F1_LR\n",
    "df_f1.F1_RF  = df_RF.F1_RF\n",
    "df_f1.F1_GRD  = df_GRD.F1_GRD\n",
    "df_f1.Max_F1 = df_f1.max(axis=1)\n",
    "\n",
    "#df_f1.to_csv('/Users/valery/Documents/Springboard/Capstone_2/Pretuned_Results/Pretuned_f1.csv')\n",
    "\n",
    "df_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categories</th>\n",
       "      <th>auc_NB</th>\n",
       "      <th>auc_SVC</th>\n",
       "      <th>auc_LR</th>\n",
       "      <th>auc_RF</th>\n",
       "      <th>auc_GRD</th>\n",
       "      <th>Max_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>0.617065</td>\n",
       "      <td>0.558591</td>\n",
       "      <td>0.578982</td>\n",
       "      <td>0.570422</td>\n",
       "      <td>0.571353</td>\n",
       "      <td>0.617065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>0.750151</td>\n",
       "      <td>0.659201</td>\n",
       "      <td>0.718866</td>\n",
       "      <td>0.713677</td>\n",
       "      <td>0.699363</td>\n",
       "      <td>0.750151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0.635560</td>\n",
       "      <td>0.616927</td>\n",
       "      <td>0.672090</td>\n",
       "      <td>0.629962</td>\n",
       "      <td>0.653992</td>\n",
       "      <td>0.672090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Categories    auc_NB   auc_SVC    auc_LR    auc_RF   auc_GRD   Max_auc\n",
       "0   positive  0.617065  0.558591  0.578982  0.570422  0.571353  0.617065\n",
       "1   negative  0.750151  0.659201  0.718866  0.713677  0.699363  0.750151\n",
       "2    neutral  0.635560  0.616927  0.672090  0.629962  0.653992  0.672090"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_auc = pd.DataFrame(columns = ['Categories', 'auc_NB', 'auc_SVC', \n",
    "                                   'auc_LR', 'auc_RF','auc_GRD','Max_auc'])\n",
    "df_auc.Categories = df_NB.Categories\n",
    "df_auc.auc_NB = df_NB.auc_NB\n",
    "df_auc.auc_SVC  = df_SVC.auc_SVC\n",
    "df_auc.auc_LR  = df_LR.auc_LR\n",
    "df_auc.auc_RF  = df_RF.auc_RF\n",
    "df_auc.auc_GRD  = df_GRD.auc_GRD\n",
    "df_auc.Max_auc = df_auc.max(axis=1)\n",
    "\n",
    "#df_auc.to_csv('/Users/valery/Documents/Springboard/Capstone_2/Pretuned_Results/Pretuned_f1.csv')\n",
    "\n",
    "df_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best auc scores for the positive and negative categories are with the tuned MultinomialNB. The tuned LogisticRegressionClassifier has the best auc scores for the neutral category. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am interested in classsifying the highest number of positive labels for each category (note that here we are using 'positive' to mean a classification of '1' versus '0', not the 'positive' label for race sentiment. For this, I maximized recall, sometimes called sensitivity, which is the true positive rate. Recall is the ratio of positive instances that are correctly detected by the classification algorithm.\n",
    "\n",
    "$Recall = \\frac{TP}{TP + FN}$\n",
    "\n",
    "This is contrasted with precision, which is the accuracy of the positive predictions.\n",
    "\n",
    "$Precision = \\frac{TP}{TP + FP}$\n",
    "\n",
    "If we were concerned only with getting the highest accuracy possible, we would tune on precision, but the cost would be that we would predict much fewer (albeit correct) positive instances. Tuning for recall will lower the accuracy, but we will lower the risk of leaving some positive instances out. The difficulty is that there is a tradeoff between recall and precision. Increasing one will reduce the other. The F1 score gives a picture of how well the classifier performs for each. It is the harmonic mean between precision and recall. This score is high for classifiers that have similar precision and recall. \n",
    "\n",
    "$F1 = 2\\frac{P \\times R}{P+R}$\n",
    "\n",
    "Finally, the area under the ROC curve (AUC) is a way to compare classifiers using above metrics. It is the area under the curve plotted as the True Positive Rate (Recall) against the False Positive Rate (the ratio of the negative instances ('0') that are incorrectly classified as positive ('1'). A perfect classifier will have an AUC = 1, and one that is purely random will equal 0.5. So, we are looking for the highest AUC score above 0.5. \n",
    "\n",
    "Since our data is highy unbalanced for each category, there will be far more negative instances than positive instances, the ROC curve isn't the best metric to use. Instead, it is better to use a Precision-Recall Curve. However, this is not easily done in Sklearn with every classifier. It depends on calculating the decision_function, which isn't available for every classifier (e.g., MultinomialNB or RandomForestClassifier).\n",
    "\n",
    "To evaluate these classifiers I need a metric that is shared among them all. Therefore I will look at the Recall, F1 and the AUC scores. Best overall score is given to the best out of 3. In the case of a tie, it will go to best recall.\n",
    "\n",
    "\n",
    "|Category|Best Recall|Best F1|Best AUC|Best Overall|\n",
    "|--------|-----------|-------|--------|------------|\n",
    "|Positive|LinearSVC|LinearSVC|MultinomialNB|LinearSVC|\n",
    "|Negative|MultinomialNB|MultinomialNB|MultinomialNB|MultinomialNB|\n",
    "|Neutral|GradientBoostingClassifier|GradientBoostingClassifier|LogisticRegression|GradientBoostingClassifier|\n",
    "\n",
    "We want to predict the highest number of tweets in each category and are willing to accept a few incorrect classifications in order to catch all the correct classifications. These metrics attend to both precision and recall so that the overall choice of classifiers for each category will minimize false negative claims."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final training with AUC scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n",
      "0.5418285278843047 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf_SVC = LinearSVC(random_state=42)\n",
    "\n",
    "loss = ['hinge', 'squared_hinge']\n",
    "    \n",
    "C = [1, 10, 100, 1000, 10000]\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C, loss=loss)\n",
    "\n",
    "# Create grid search using 5-fold cross validation\n",
    "clf_tune = GridSearchCV(clf_SVC, hyperparameters, cv=5, scoring='recall')   \n",
    "\n",
    "category = ['positive']\n",
    "\n",
    "for cat in category:\n",
    "    # Fit grid search\n",
    "    best_model = clf_tune.fit(Xv, df_train[cat])                \n",
    "    \n",
    "    SVCloss = best_model.best_estimator_.get_params()['loss']\n",
    "    SVC_C = best_model.best_estimator_.get_params()['C']\n",
    "    \n",
    "    clf_tunedSVC = LinearSVC(random_state=42, loss=SVCloss, C=SVC_C)\n",
    "\n",
    "    y = df_train[category].values.astype(np.int)\n",
    "\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Xv, y, test_size=0.3)\n",
    "\n",
    "    # Supervised transformation\n",
    "    #clf_tunedSVC = LinearSVC(random_state=42, loss='squared_hinge', C=1000)\n",
    "    clf_tunedSVC.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clf_tunedSVC.predict(X_test)\n",
    "    fprSVC, tprSVC, _ = roc_curve(y_test, y_pred)\n",
    "    aucSVC = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    print(cat)\n",
    "    print(aucSVC, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n",
      "0.7392679221283962 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf_NB = MultinomialNB()\n",
    "\n",
    "alphas = [.00001,.0001, .001, .01, .1, 1, 5, 10, 50]\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(alpha=alphas)\n",
    "\n",
    "# Create grid search using 5-fold cross validation\n",
    "clf_tune = GridSearchCV(clf_NB, hyperparameters, cv=5, scoring='recall')               \n",
    "\n",
    "category = ['negative']\n",
    "\n",
    "for cat in category:\n",
    "    # Fit grid search\n",
    "    best_model = clf_tune.fit(Xv, df_train[cat])                \n",
    "    alpha = best_model.best_estimator_.get_params()['alpha']\n",
    "    \n",
    "    clf_tunedNB = MultinomialNB(alpha=alpha)\n",
    "\n",
    "    y = df_train[cat].values.astype(np.int)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Xv, y, test_size=0.3)\n",
    "\n",
    "    # Supervised transformation based on gradient boosted trees\n",
    "    clf_tunedNB.fit(X_train, y_train)\n",
    "    \n",
    "    # Prediction and predicted AUC score\n",
    "    y_pred = clf_tunedNB.predict(X_test)\n",
    "    fprNB, tprNB, _ = roc_curve(y_test, y_pred)\n",
    "    aucNB = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    print(cat)\n",
    "    print(aucNB, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral\n",
      "0.6475611066344344 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_GRD = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "loss=['deviance', 'exponential']\n",
    "n_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200]\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(loss=loss, n_estimators=n_estimators)\n",
    "\n",
    "# Create grid search using 5-fold cross validation\n",
    "clf_tune = GridSearchCV(clf_GRD, hyperparameters, cv=5, scoring='recall')\n",
    "\n",
    "category = ['neutral']\n",
    "\n",
    "for cat in category:\n",
    "    # Fit grid search\n",
    "    best_model = clf_tune.fit(Xt, df_train[category]) \n",
    "    GRDloss = best_model.best_estimator_.get_params()['loss']\n",
    "    GRDestimators = best_model.best_estimator_.get_params()['n_estimators']\n",
    "                 \n",
    " \n",
    "    clf_tunedGRD= GradientBoostingClassifier(random_state=42, loss=GRDloss, n_estimators=GRDestimators)\n",
    "    \n",
    "    #Make training and testing data.\n",
    "    y = df_train[category].values.astype(np.int)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(Xt, y, test_size=0.3)\n",
    "    \n",
    "    #Fit the tuned classifier to the training data.\n",
    "    clf_tunedGRD.fit(X_train, y_train)\n",
    "\n",
    "    #Calculate AUC score with the test set\n",
    "    y_pred = clf_tunedGRD.predict(X_test)\n",
    "    fprGRD, tprGRD, _ = roc_curve(y_test, y_pred)\n",
    "    aucGRD = roc_auc_score(y_test, y_pred)\n",
    "   \n",
    "    print(cat)\n",
    "    print(aucGRD, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
