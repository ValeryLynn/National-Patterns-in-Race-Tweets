{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-label classification of racial tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6481, 5)\n",
      "\n",
      "Index(['tweetidg', 'tweet', 'positive', 'negative', 'neutral'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetidg</th>\n",
       "      <th>tweet</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>588687492888551424g</td>\n",
       "      <td>i am the type of nigga tryna get rich</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>592553981601304576g</td>\n",
       "      <td>sheblasiannn smuckers</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>717371522956984320g</td>\n",
       "      <td>proteinwisdom we even make japanese cars and e...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>590292541125328898g</td>\n",
       "      <td>i wanted to see my nigga when i went back home...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>592522044480225282g</td>\n",
       "      <td>wth stevo is that a nigga</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweetidg                                              tweet  \\\n",
       "0  588687492888551424g              i am the type of nigga tryna get rich   \n",
       "1  592553981601304576g                              sheblasiannn smuckers   \n",
       "2  717371522956984320g  proteinwisdom we even make japanese cars and e...   \n",
       "3  590292541125328898g  i wanted to see my nigga when i went back home...   \n",
       "4  592522044480225282g                          wth stevo is that a nigga   \n",
       "\n",
       "   positive  negative  neutral  \n",
       "0       0.0       0.0      1.0  \n",
       "1       0.0       0.0      1.0  \n",
       "2       0.0       0.0      1.0  \n",
       "3       0.0       1.0      0.0  \n",
       "4       0.0       0.0      1.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read in csv of cleaned data\n",
    "df = pd.read_csv('df_tweets.csv', index_col=0)\n",
    "\n",
    "#Create training set\n",
    "\n",
    "df_train = df[df['tweet'].notnull()]\n",
    "print(df_train.shape)\n",
    "print('')\n",
    "print(df_train.columns)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a table to examine the distribution of tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Category  Number_of_Tweets    priors\n",
      "0  positive            1642.0  0.253356\n",
      "1  negative            1977.0  0.305046\n",
      "2   neutral            2862.0  0.441599\n",
      "\n",
      "Total number of labels:   6481.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_sparse = df_train.drop(['tweetidg', 'tweet'], axis=1)\n",
    "\n",
    "counts = []\n",
    "priors = []\n",
    "categories = list(df_sparse.columns.values)\n",
    "for i in categories:\n",
    "    numarts = df_sparse[i].sum()\n",
    "    counts.append((i, numarts))\n",
    "\n",
    "df_stats = pd.DataFrame(counts, columns=['Category', 'Number_of_Tweets'])\n",
    "total = df_stats.Number_of_Tweets.sum()\n",
    "\n",
    "df_stats['priors'] = df_stats['Number_of_Tweets']/total\n",
    "\n",
    "print(df_stats)\n",
    "print('')\n",
    "print('Total number of labels:  ', total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-label classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweetidg', 'tweet', 'positive', 'negative', 'neutral'], dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure all entries in df_matrix['text'] are strings\n",
    "df_train['tweet'] = df_train['tweet'].astype(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Function for positive labels\n",
    "def make_x(df, vectorizer=None):\n",
    "    if vectorizer is None:\n",
    "        vectorizer = CountVectorizer(stop_words='english',binary=False,max_df=50,\n",
    "                                     min_df=1,strip_accents='unicode',max_features=200)\n",
    "    X = vectorizer.fit_transform(df_train.tweet)\n",
    "    \n",
    "    #X = X.tocsc()  # some versions of sklearn return COO format\n",
    "    return type(X)\n",
    "#Call the function: X\n",
    "make_x(df_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#Function for negative labels\n",
    "def make_xy_neg(df, vectorizer=None):\n",
    "    if vectorizer is None:\n",
    "        vectorizer = CountVectorizer(stop_words='english',binary=False,max_df=25,\n",
    "                                     min_df=1,strip_accents='unicode',max_features=100)\n",
    "    X = vectorizer.fit_transform(df_train.tweet)\n",
    "    X = X.tocsc()  # some versions of sklearn return COO format\n",
    "    y = df_train['negative'].values.astype(np.int)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#Function for neutral labels\n",
    "def make_xy_neut(df, vectorizer=None):\n",
    "    if vectorizer is None:\n",
    "        vectorizer = CountVectorizer(stop_words='english',binary=False,max_df=25,\n",
    "                                     min_df=1,strip_accents='unicode',max_features=100)\n",
    "    X = vectorizer.fit_transform(df_train.tweet)\n",
    "    X = X.tocsc()  # some versions of sklearn return COO format\n",
    "    y = df_train['neutral'].values.astype(np.int)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-c3f5cb17222c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#Positive scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0myp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'positive'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mX_trainp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_testp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trainp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_testp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mclf_NB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trainp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_trainp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english',binary=False,max_df=50,\n",
    "                                     min_df=1,strip_accents='unicode',max_features=200)\n",
    "X = vectorizer.fit_transform(df_train.tweet)\n",
    "    \n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "clf_NB = MultinomialNB()\n",
    "\n",
    "#Positive scores\n",
    "yp = df_train['positive'].values.astype(np.int)\n",
    "X_trainp, X_testp, y_trainp, y_testp = train_test_split(X, yp, test_size=0.30, random_state=42)\n",
    "\n",
    "clf_NB.fit(X_trainp,y_trainp)\n",
    "\n",
    "train_accuracy_pos =clf_NB.score(X_trainp,y_trainp)\n",
    "test_accuracy_pos = clf_NB.score(X_testp, y_testp)\n",
    "print(\"Positive - training set accuracy: \", train_accuracy_pos)\n",
    "print(\"Positive - test set accuracy: \", test_accuracy_pos, '\\n')\n",
    "        \n",
    "#Negative scores\n",
    "yn = df_train['negative'].values.astype(np.int)\n",
    "X_trainn, X_testn, y_trainn, y_testn = train_test_split(X, yn, test_size=0.30, random_state=42)\n",
    "\n",
    "clf_NB.fit(X_trainn,y_trainn)\n",
    "\n",
    "train_accuracy_neg = clf_NB.score(X_trainn, y_trainn)\n",
    "test_accuracy_neg = clf_NB.score(X_testn, y_testn)\n",
    "print(\"Negative - training set accuracy: \", train_accuracy_neg)\n",
    "print(\"Negative - test set accuracy: \", test_accuracy_neg, '\\n')\n",
    "        \n",
    "#Neutral scores\n",
    "yne = df_train['neutral'].values.astype(np.int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, yne, test_size=0.30, random_state=42)\n",
    "\n",
    "clf_NB.fit(X_train,y_train)\n",
    "\n",
    "train_accuracy_neut = clf_NB.score(X_train, y_train)\n",
    "test_accuracy_neut = clf_NB.score(X_test, y_test)\n",
    "print(\"Neutral - training set accuracy: \", train_accuracy_neut)\n",
    "print(\"Neutral - test set accuracy: \", test_accuracy_neut)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is looking good! But accuracy can be deceiving. High accuracy scores can still happen if your model detects many true negatives, but no try positives! Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "#Prediciton with NB\n",
    "conmatrxNB = []\n",
    "precisionNB = []\n",
    "recallNB = []\n",
    "f1NB = []\n",
    "\n",
    "\n",
    "for category in categories:\n",
    "\n",
    "    y_train_pred = cross_val_predict(clf_NB, X, df_train[category], cv=5)\n",
    "    \n",
    "    cmNB = confusion_matrix(df_train[category], y_train_pred)\n",
    "    conmatrxNB.append(cmNB)\n",
    "    precNB = precision_score(df_train[category], y_train_pred)    \n",
    "    precisionNB.append(precNB)\n",
    "    recNB = recall_score(df_train[category], y_train_pred)\n",
    "    recallNB.append(recNB)\n",
    "    fNB = f1_score(df_train[category], y_train_pred)\n",
    "    f1NB.append(fNB)\n",
    "\n",
    "    \n",
    "dictNB = {'Categories':categories, 'Confusion Matrix':conmatrxNB, 'Precision_NB':precisionNB, 'Recall':recallNB, 'F1':f1NB}\n",
    "df_NB = pd.DataFrame(dictNB) \n",
    "#df_NB.to_csv('Results_NB_tweets.csv')\n",
    "\n",
    "df_NB_tweets = df_NB[df_NB['Precision_NB'] >= 0]\n",
    " \n",
    "df_NB_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I expected, high accuracy doesn't always mean high precision or recall. We can see that these are lower. I will run the other models first, then tune parameters on the one showing the most promise for this data. Note that for the neutral label, accuracy was lower than precision. For this study, we want to decrease the probability of a Type 2 error - a false positive where we say there is an effect but there is not. Precision is the best score to tune because it is a measure of how sure we are that a label is a true label (true positive) if our algorithm classifies it as such. After running the other two classifiers, I will tune the hyper-parameters for the best performing classifier at these default settings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "clf_SVC = OneVsRestClassifier(LinearSVC(), n_jobs=1)\n",
    "\n",
    "#Positive scores\n",
    "yp = df_train['positive'].values.astype(np.int)\n",
    "X_trainp, X_testp, y_trainp, y_testp = train_test_split(X, yp, test_size=0.30, random_state=42)\n",
    "\n",
    "clf_SVC.fit(X_trainp,y_trainp)\n",
    "\n",
    "train_accuracy_pos =clf_SVC.score(X_trainp,y_trainp)\n",
    "test_accuracy_pos = clf_SVC.score(X_testp, y_testp)\n",
    "print(\"Positive - training set accuracy: \", train_accuracy_pos)\n",
    "print(\"Positive - test set accuracy: \", test_accuracy_pos, '\\n')\n",
    "        \n",
    "#Negative scores\n",
    "yn = df_train['negative'].values.astype(np.int)\n",
    "X_trainn, X_testn, y_trainn, y_testn = train_test_split(X, yn, test_size=0.30, random_state=42)\n",
    "\n",
    "clf_SVC.fit(X_trainn,y_trainn)\n",
    "\n",
    "train_accuracy_neg = clf_SVC.score(X_trainn, y_trainn)\n",
    "test_accuracy_neg = clf_SVC.score(X_testn, y_testn)\n",
    "print(\"Negative - training set accuracy: \", train_accuracy_neg)\n",
    "print(\"Negative - test set accuracy: \", test_accuracy_neg, '\\n')\n",
    "        \n",
    "#Neutral scores\n",
    "yne = df_train['neutral'].values.astype(np.int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, yne, test_size=0.30, random_state=42)\n",
    "\n",
    "clf_SVC.fit(X_train,y_train)\n",
    "\n",
    "train_accuracy_neut = clf_SVC.score(X_train, y_train)\n",
    "test_accuracy_neut = clf_SVC.score(X_test, y_test)\n",
    "print(\"Neutral - training set accuracy: \", train_accuracy_neut)\n",
    "print(\"Neutral - test set accuracy: \", test_accuracy_neut)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_SVC = LinearSVC()\n",
    "\n",
    "\n",
    "#Prediciton with NB\n",
    "conmatrxSVC = []\n",
    "precisionSVC = []\n",
    "recallSVC = []\n",
    "f1SVC = []\n",
    "\n",
    "\n",
    "\n",
    "for category in categories:\n",
    "\n",
    "    y_train_pred = cross_val_predict(clf_SVC, X, df_train[category], cv=5)\n",
    "    \n",
    "    cmSVC = confusion_matrix(df_train[category], y_train_pred)\n",
    "    conmatrxSVC.append(cmSVC)\n",
    "    precSVC = precision_score(df_train[category], y_train_pred)    \n",
    "    precisionSVC.append(precSVC)\n",
    "    recSVC = recall_score(df_train[category], y_train_pred)\n",
    "    recallSVC.append(recSVC)\n",
    "    fSVC = f1_score(df_train[category], y_train_pred)\n",
    "    f1SVC.append(fSVC)\n",
    "\n",
    "    \n",
    "dictSVC = {'Categories':categories, 'Confusion Matrix':conmatrxSVC, 'Precision_SVC':precisionSVC, \n",
    "           'Recall':recallSVC, 'F1':f1SVC}\n",
    "df_SVC = pd.DataFrame(dictSVC) \n",
    "#df_NB.to_csv('Results_NB_tweets.csv')\n",
    "\n",
    "df_SVC_tweets = df_SVC[df_SVC['Precision_SVC'] >= 0]\n",
    " \n",
    "df_SVC_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_LR = LogisticRegression()\n",
    "\n",
    "\n",
    "#Prediciton with NB\n",
    "conmatrxLR = []\n",
    "precisionLR = []\n",
    "recallLR = []\n",
    "f1LR = []\n",
    "\n",
    "\n",
    "\n",
    "for category in categories:\n",
    "\n",
    "    y_train_pred = cross_val_predict(clf_LR, X, df_train[category], cv=5)\n",
    "    \n",
    "    cmLR = confusion_matrix(df_train[category], y_train_pred)\n",
    "    conmatrxLR.append(cmLR)\n",
    "    precLR = precision_score(df_train[category], y_train_pred)    \n",
    "    precisionLR.append(precLR)\n",
    "    recLR = recall_score(df_train[category], y_train_pred)\n",
    "    recallLR.append(recLR)\n",
    "    fLR = f1_score(df_train[category], y_train_pred)\n",
    "    f1LR.append(fLR)\n",
    "\n",
    "    \n",
    "dictLR = {'Categories':categories, 'Confusion Matrix':conmatrxLR, 'Precision_LR':precisionLR, \n",
    "           'Recall':recallLR, 'F1':f1LR}\n",
    "df_LR = pd.DataFrame(dictLR) \n",
    "#df_NB.to_csv('Results_NB_tweets.csv')\n",
    "\n",
    "df_LR_tweets = df_LR[df_LR['Precision_LR'] >= 0]\n",
    " \n",
    "df_LR_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = df_NB.Categories\n",
    "df_prec_NB  =df_NB.Precision_NB\n",
    "df_prec_SVC  = df_SVC.Precision_SVC\n",
    "df_prec_LR  = df_LR.Precision_LR\n",
    "\n",
    "df_prec1 = pd.DataFrame(columns = ['Categories', 'Precision_NB', 'Precision_SVC', \n",
    "                                   'Precision_LR', 'Max_Precision'])\n",
    "df_prec1.Categories = df_NB.Categories\n",
    "df_prec1.Precision_NB = df_NB.Precision_NB\n",
    "df_prec1.Precision_SVC  = df_SVC.Precision_SVC\n",
    "df_prec1.Precision_LR  = df_LR.Precision_LR\n",
    "df_prec1.Max_Precision = df_prec1.max(axis=1)\n",
    "df_prec1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune hyperparameters for multi\n",
    "\n",
    "The ‘newton-cg’, ‘sag’ and ‘lbfgs’ solvers support only l2 penalties. I will first leave out the solver and check for the best penalty. The I will choose the solvers that best run with that penalty.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GridSearchCV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3539afb11b4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m      \u001b[0;31m# Create grid search using 5-fold cross validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mclf_tune\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf_LR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f1_score'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcategory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GridSearchCV' is not defined"
     ]
    }
   ],
   "source": [
    "clf_LR = LogisticRegression()\n",
    "\n",
    " \n",
    "#The ‘newton-cg’, ‘sag’ and ‘lbfgs’ solvers support only l2 penalties.\n",
    "\n",
    "    # Create regularization penalty space\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "    # Create regularization hyperparameter space\n",
    "C = np.logspace(-4, 4, 20)\n",
    "\n",
    "#solver = ['liblinear', 'sag', 'newton-cg', 'saga', 'lbfgs']\n",
    "\n",
    "\n",
    "    # Create hyperparameter options\n",
    "hyperparameters = dict(penalty = penalty, C=C)\n",
    "\n",
    "\n",
    "     # Create grid search using 5-fold cross validation\n",
    "clf_tune = GridSearchCV(clf_LR, hyperparameters, cv=5, verbose=0, scoring='f1_score')               \n",
    "\n",
    "for category in categories:\n",
    "    # Fit grid search\n",
    "    best_model = clf_tune.fit(X, df_train[category])                \n",
    "                \n",
    "    # View best hyperparameters\n",
    "    print(category)\n",
    "    print('Best penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "    print('Best C:', best_model.best_estimator_.get_params()['C'],'\\n')         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is obvious that each label will need to be individually tuned. Let's check for the solver under the l2 penalty. We need not check l1 because liblinear is both the default and the only solver that will work with l1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n",
      "Best solver: liblinear\n",
      "Best C: 0.23357214690901212 \n",
      "\n",
      "negative\n",
      "Best solver: liblinear\n",
      "Best C: 0.08858667904100823 \n",
      "\n",
      "neutral\n",
      "Best solver: sag\n",
      "Best C: 0.03359818286283781 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#The ‘newton-cg’, ‘sag’ and ‘lbfgs’ solvers support only l2 penalties.\n",
    "\n",
    "    # Create regularization penalty space\n",
    "penalty = ['l2']\n",
    "\n",
    "    # Create regularization hyperparameter space\n",
    "C = np.logspace(-4, 4, 20)\n",
    "\n",
    "solver = ['liblinear', 'sag', 'newton-cg', 'saga', 'lbfgs']\n",
    "\n",
    "    # Create hyperparameter options\n",
    "hyperparameters = dict(penalty=penalty,C=C, solver=solver)\n",
    "\n",
    "\n",
    "     # Create grid search using 5-fold cross validation\n",
    "clf_tune = GridSearchCV(clf_LR, hyperparameters, cv=5, verbose=0, scoring='precision')               \n",
    "\n",
    "for category in categories:\n",
    "    # Fit grid search\n",
    "    best_model = clf_tune.fit(X, df_train[category])                \n",
    "                \n",
    "    # View best hyperparameters\n",
    "    print(category)\n",
    "    print('Best solver:', best_model.best_estimator_.get_params()['solver'])\n",
    "    print('Best C:', best_model.best_estimator_.get_params()['C'],'\\n')         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, we have a different combination for each label. I will run each separately and examine the differences before predicting on the rately and examine the differences before predicting on the unlabeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the tuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categories</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "      <th>Precision_LR</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>[[4835, 4], [1624, 18]]</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.010962</td>\n",
       "      <td>0.021635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>[[4501, 3], [1943, 34]]</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.017198</td>\n",
       "      <td>0.033764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>[[3614, 5], [2827, 35]]</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.012229</td>\n",
       "      <td>0.024121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Categories         Confusion Matrix  Precision_LR    Recall        F1\n",
       "0   positive  [[4835, 4], [1624, 18]]      0.818182  0.010962  0.021635\n",
       "1   negative  [[4501, 3], [1943, 34]]      0.918919  0.017198  0.033764\n",
       "2    neutral  [[3614, 5], [2827, 35]]      0.875000  0.012229  0.024121"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hyperpos = ['C=0.23357214690901212']\n",
    "#hyperneg = ['penalty=l2, C=0.08858667904100823']\n",
    "#hyperneut = [\"penalty=l2, C=0.03359818286283781, solver='sag'\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Prediciton with NB\n",
    "conmatrxLR = []\n",
    "precisionLR = []\n",
    "recallLR = []\n",
    "f1LR = []\n",
    "\n",
    "\n",
    "for category in categories:\n",
    "    if category == 'positive':\n",
    "        clf_tuned = LogisticRegression(penalty='l1',C=0.23357214690901212,  solver='liblinear')\n",
    "    if category == 'negative':\n",
    "        clf_tuned = LogisticRegression(penalty='l2', C=0.08858667904100823, solver='liblinear')\n",
    "    if category == 'neutral':\n",
    "        clf_tuned = LogisticRegression(penalty='l2', C=0.03359818286283781, solver='sag')\n",
    "    \n",
    "    y_train_pred = cross_val_predict(clf_tuned, X, df_train[category], cv=5)\n",
    "    \n",
    "    cmLR = confusion_matrix(df_train[category], y_train_pred)\n",
    "    conmatrxLR.append(cmLR)\n",
    "    precLR = precision_score(df_train[category], y_train_pred)    \n",
    "    precisionLR.append(precLR)\n",
    "    recLR = recall_score(df_train[category], y_train_pred)\n",
    "    recallLR.append(recLR)\n",
    "    fLR = f1_score(df_train[category], y_train_pred)\n",
    "    f1LR.append(fLR)\n",
    "\n",
    "dictLR = {'Categories':categories, 'Confusion Matrix':conmatrxLR, 'Precision_LR':precisionLR, \n",
    "           'Recall':recallLR, 'F1':f1LR}\n",
    "df_LR = pd.DataFrame(dictLR) \n",
    "#df_NB.to_csv('Results_NB_tweets.csv')\n",
    "\n",
    "df_LR_tweets = df_LR[df_LR['Precision_LR'] >= 0]\n",
    " \n",
    "df_LR_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4835,    4],\n",
       "       [1624,   18]])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " conmatrxLR[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
